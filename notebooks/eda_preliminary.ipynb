{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary EDA - Data Quality Hell\n",
    "\n",
    "This notebook covers the initial inspection and cleaning of the **Model Case** dataset (January 1st - 15th, 2026). \n",
    "\n",
    "**Objective:** Prepare the data for deeper Exploratory Data Analysis (EDA) and future transformations using a stable, reproducible snapshot."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We start with the merged **Model Case** dataset containing 4,483 records from 6 countries extracted for the Jan 1-15 period."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "input_csv = Path(\"../data/interim/all_jobs_merged.csv\")\n",
    "df = pd.read_csv(input_csv)\n",
    "print(f\"Initial Shape: {df.shape}\")\n",
    "df.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Shape: (4483, 8)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Drop Unnecessary Columns\n",
    "\n",
    "Columns `description` (too inconsistent) and `adref` (no analytical value) are dropped to simplify the analysis.\n",
    "\n",
    "> **Note:** We use `errors='ignore'` so the cell can be re-run without errors if the columns were already removed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cols_to_drop = ['description', 'adref']\n",
    "df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "df.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Date Conversion\n",
    "\n",
    "Converting the `created` column to a standard `datetime` format. We use `errors='coerce'` to handle any malformed strings."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df['created'] = pd.to_datetime(df['created'], errors='coerce')\n",
    "print(f\"Missing dates after conversion: {df['created'].isnull().sum()}\")\n",
    "df.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing dates after conversion: 0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4483 entries, 0 to 4482\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype              \n",
      "---  ------        --------------  -----              \n",
      " 0   country_code  4483 non-null   object             \n",
      " 1   title         4483 non-null   object             \n",
      " 2   id            4483 non-null   int64              \n",
      " 3   company       4065 non-null   object             \n",
      " 4   location      4483 non-null   object             \n",
      " 5   created       4483 non-null   datetime64[ns, UTC]\n",
      "dtypes: datetime64[ns, UTC](1), int64(1), object(4)\n",
      "memory usage: 210.3+ KB\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Null Analysis\n",
    "\n",
    "Identifying columns with missing values and inspecting the problematic rows."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "null_counts = df.isnull().sum()\n",
    "print(\"--- Null Counts per Column ---\")\n",
    "print(null_counts)\n",
    "\n",
    "print(\"\\n--- Rows with Null Title ---\")\n",
    "if 'title' in df.columns:\n",
    "    display(df[df['title'].isnull()])\n",
    "\n",
    "print(\"\\n--- Sample of Rows with Null Company (First 10) ---\")\n",
    "if 'company' in df.columns:\n",
    "    display(df[df['company'].isnull()].head(10))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Null Counts per Column ---\n",
      "country_code      0\n",
      "title             0\n",
      "id                0\n",
      "company         418\n",
      "location          0\n",
      "created           0\n",
      "dtype: int64\n",
      "\n",
      "--- Rows with Null Title ---\n",
      "Empty DataFrame\n",
      "Columns: [country_code, title, id, company, location, created]\n",
      "Index: []\n",
      "\n",
      "--- Sample of Rows with Null Company (First 10) ---\n",
      "     country_code  \\\n",
      "2764           nl   \n",
      "3652           sg   \n",
      "3653           sg   \n",
      "3654           sg   \n",
      "3655           sg   \n",
      "3656           sg   \n",
      "3657           sg   \n",
      "3658           sg   \n",
      "3659           sg   \n",
      "3660           sg   \n",
      "\n",
      "                                                                                                   title  \\\n",
      "2764                                                                         Growth Marketeer | Skincare   \n",
      "3652                                                                      Business Development Executive   \n",
      "3653                                            Sales & Marketing Executive (Japanese Speaking, Support)   \n",
      "3654                                                                  Full Time / Locum Dental Assistant   \n",
      "3655                                                                             EUC Deployment Engineer   \n",
      "3656                                                                          Sales Engineer (Pre-sales)   \n",
      "3657                                                                      Content Creator (F&B Industry)   \n",
      "3658                                                                         Planning Coordinator - WCAN   \n",
      "3659  Senior Learning Data Specialist/Learning Data Specialist (Centre for Innovation in Learning) [NIE]   \n",
      "3660                                                                         Corporate Finance Associate   \n",
      "\n",
      "              id company               location                   created  \n",
      "2764  5586989814     NaN  Overveen, Bloemendaal 2026-01-15 22:24:47+00:00  \n",
      "3652  5585723038     NaN              Singapore 2026-01-15 13:57:31+00:00  \n",
      "3653  5585722999     NaN              Singapore 2026-01-15 13:57:30+00:00  \n",
      "3654  5585722992     NaN              Singapore 2026-01-15 13:57:30+00:00  \n",
      "3655  5585722978     NaN              Singapore 2026-01-15 13:57:30+00:00  \n",
      "3656  5585722984     NaN              Singapore 2026-01-15 13:57:30+00:00  \n",
      "3657  5585722996     NaN              Singapore 2026-01-15 13:57:30+00:00  \n",
      "3658  5585723026     NaN              Singapore 2026-01-15 13:57:30+00:00  \n",
      "3659  5585722989     NaN              Singapore 2026-01-15 13:57:30+00:00  \n",
      "3660  5585723024     NaN              Singapore 2026-01-15 13:57:30+00:00  \n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:**\n",
    "- **General Scope:** The dataset contains **4,483 jobs** across 6 countries for the Jan 1-15 period.\n",
    "- **Country Distribution:** Switzerland (1,163), Belgium (920), and Singapore (845) are the most represented.\n",
    "- **Quality Metrics:**\n",
    "    - **Titles:** Perfect coverage (0 nulls).\n",
    "    - **Companies:** 418 missing values (9.3%), concentrated in Singapore and Netherlands entries.\n",
    "    - **Dates:** All dates fall within the target range (Jan 1st - 15th).\n",
    "\n",
    "**Proposed Strategy:** Fill missing companies with \"Unknown\". The data is remarkably clean regarding titles and dates, allowing us to move forward without dropping rows at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Duplicate Analysis\n",
    "\n",
    "Checking for exact row duplicates across the key remaining columns."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"\\n--- Sample of Duplicated Rows ---\")\n",
    "    display(df[df.duplicated()].head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows: 0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:** No exact row duplicates were found in this snapshot. \n",
    "\n",
    "**Rationale:** This suggests that the Adzuna API retrieval and the subsequent flattening/merging scripts are correctly handling pagination without overlapping records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initial Cleaning (Execution)\n",
    "\n",
    "Applying the decisions made above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Remove null title\n",
    "df = df.dropna(subset=['title'])\n",
    "\n",
    "# Fill null companies\n",
    "df['company'] = df['company'].fillna('Unknown')\n",
    "\n",
    "print(f\"Final Shape after cleaning: {df.shape}\")\n",
    "df.isnull().sum()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Shape after cleaning: (4483, 6)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Job Content Analysis:** Perform a frequency analysis of job titles to identify common roles.\n",
    "2. **Geographical Distribution:** Visualize job density across the 6 represented countries.\n",
    "3. **Time Series Exploration:** Analyze daily job posting counts to identify trends in early January.\n",
    "4. **Company Profiling:** identify the top recruiters in this dataset (once \"Unknown\" are handled)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}