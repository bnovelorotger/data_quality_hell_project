{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary EDA - Data Quality Hell\n",
    "\n",
    "This notebook covers the initial inspection and cleaning of the **Model Case** dataset (January 1st - 15th, 2026). \n",
    "\n",
    "**Objective:** Prepare the data for deeper Exploratory Data Analysis (EDA) and future transformations using a stable, reproducible snapshot."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We start with the merged **Model Case** dataset containing 39,844 records from 19 countries extracted for the Jan 1-15 period."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "input_csv = Path(\"../data/interim/all_jobs_merged.csv\")\n",
    "df = pd.read_csv(input_csv)\n",
    "print(f\"Initial Shape: {df.shape}\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Drop Unnecessary Columns\n",
    "\n",
    "Columns `description` (too inconsistent) and `adref` (no analytical value) are dropped to simplify the analysis.\n",
    "\n",
    "> **Note:** We use `errors='ignore'` so the cell can be re-run without errors if the columns were already removed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cols_to_drop = ['description', 'adref']\n",
    "df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Date Conversion\n",
    "\n",
    "Converting the `created` column to a standard `datetime` format. We use `errors='coerce'` to handle any malformed strings."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df['created'] = pd.to_datetime(df['created'], errors='coerce')\n",
    "print(f\"Missing dates after conversion: {df['created'].isnull().sum()}\")\n",
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:**\n",
    "- **General Scope:** The dataset contains **39,844 jobs** across 19 countries for the Jan 1-15 period.\n",
    "- **Technical Depth:** Captures specialized roles (Data Engineer, Scientist, Analyst, MLOps, Architect).\n",
    "- **Quality Metrics:**\n",
    "    - **Titles:** Perfect coverage (0 nulls).\n",
    "    - **Companies:** 1,333 missing values (3.3%), appearing across multiple territories.\n",
    "    - **Dates:** All dates fall within the target range (Jan 1st - 15th).\n",
    "\n",
    "**Proposed Strategy:** Fill missing companies with \"Unknown\". We also have a `search_term` column for 35,361 records to enable role-based segment analysis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "null_counts = df.isnull().sum()\n",
    "print(\"--- Null Counts per Column ---\")\n",
    "print(null_counts)\n",
    "\n",
    "print(\"\\n--- Sample of Rows with Null Company (First 10) ---\")\n",
    "if 'company' in df.columns:\n",
    "    display(df[df['company'].isnull()].head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Status Check:** The data is mostly clean regarding mandatory fields (Title, Date, Location). The next step is to address internal consistency and redundant records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Role & Record Redundancy Analysis\n",
    "\n",
    "Checking for exact row duplicates and unique job identifier matches."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "exact_duplicates = df.duplicated().sum()\n",
    "id_duplicates = df.duplicated(subset=['id']).sum()\n",
    "\n",
    "print(f\"Exact row duplicates: {exact_duplicates}\")\n",
    "print(f\"Duplicate job IDs: {id_duplicates}\")\n",
    "\n",
    "if id_duplicates > 0:\n",
    "    print(\"\\n--- Sample of Rows with Duplicate IDs ---\")\n",
    "    # Show some examples of duplicated IDs to understand why they exist\n",
    "    duplicate_ids = df[df.duplicated(subset=['id'])]['id'].head(3)\n",
    "    display(df[df['id'].isin(duplicate_ids)].sort_values(by='id').head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:** While there are no exact row duplicates, we found **11,819 duplicate job IDs**.\n",
    "\n",
    "**Revised Strategy:** Instead of treating these as 'junk' duplicates, we recognize that the same job can match multiple search terms (e.g., 'Data Engineer' and 'Big Data'). \n",
    "\n",
    "To preserve the richness of the classification, we will **not** drop these records yet. This allows us to perform a more accurate 'Market Demand' analysis by role in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initial Cleaning (Execution)\n",
    "\n",
    "Applying the decisions made above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Note: We are KEEPING duplicate IDs to preserve multi-role classification\n",
    "# as decided in the Data Quality strategy update.\n",
    "\n",
    "# Remove null title (safety check)\n",
    "df = df.dropna(subset=['title'])\n",
    "\n",
    "# Fill null companies\n",
    "df['company'] = df['company'].fillna('Unknown')\n",
    "\n",
    "print(f\"Final Shape after cleaning: {df.shape}\")\n",
    "print(f\"Total Unique Jobs (by ID): {df['id'].nunique()}\")\n",
    "print(f\"\\nRemaining missing values:\\n{df.isnull().sum()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Job Content Analysis:** Perform a frequency analysis of job titles to identify common roles.\n",
    "2. **Geographical Distribution:** Visualize job density across the different countries.\n",
    "3. **Time Series Exploration:** Analyze daily job posting counts to identify trends in early January.\n",
    "4. **Company Profiling:** identify the top recruiters in this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}